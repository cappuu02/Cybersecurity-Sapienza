Raft primary backup system (Apply the primary backup operation) that work in eventually synchronous system with majority of correct process.

>Creato perchè l'algoritmo per risolvere l'active backup era molto complesso e dispendioso.

Raft a consensus algorithm for replicated logs.

**Goal**: Create a replicated log (is an append only list of command). We have an object (tree, computer, and we want to do operation on this objects). We have different replica's of this objects. Each one has a distributed log to put the operations. Raft is the module that keep the distributed log cconsistent on each replica. This means that position 2 of replica 2 is the same of position 2 of replica 1 and so on.
When there is an operation on the log i execute this on a state machine (algorith, state machine). So we have a complete turing machine.

![[Pasted image 20250130152351.png]]
- Replicated log $\to$ replicated state machine
- All servers execute same commands in same order
- Consensus module ensures proper log replication
- System makes progress as long as any majority of servers are up
- Failure model: fail-stop (not Byzantine), delayed/lost messages

>If the system become asynchronous $\to$ Raft stop working
## Overview
![[Pasted image 20250130152607.png]]

## Server states
At any time each server is either:
- **Leader**: handles all client interactions, log replication. At most $1$ viable leader at a time.
- **Follower**: completely passive (issues no RPCs, responds to incoming RPCs) (is the Backups).
- **Candidate**: used to elect a new leader.

==Normal operation==: $1$ leader, $N-1$ followers
![[Pasted image 20250130152807.png]]

Candidate, is like a follower, è qualcuno che non vede il leader per molto tempo (non vede una sua heart beat request) e dunque, dopo un certo tempo, avvia una elezione. Il candidato è qualcuno che partecipa alle elezioni e si propone come leader.

> Questo lo fanno i sistemi

## Time
![[Pasted image 20250131095731.png]]
- Time divided into terms:
	- **Election phase**
	- **Normal operation under a single leader**
- At most $1$ leader per term (not possible to have $2$ leader in the same term, only one)
- Some terms have no leader (failed election)
- **Each server maintains current term value**
- Key role of terms: identify obsolete information

>When leader dies, we go to the next term with a new election phase $\to$ new leader


##  Election Start: Heartbeats and Timeouts
I server si avviano come follower. I seguaci si aspettano di ricevere RPC dai leader o dai candidati. I leader devono inviare heartbeat (RPC AppendEntries vuoti) per mantenere l'autorità.
Se l'electionTimeout trascorre senza RPC:
- Follower assumes leader has crashed
- Follower starts new election
- Timeouts typically 100-500ms

>Heartbeat in Raft is request $Append(\bot)$.

![[293.png]]

### Election Basics
- Increment current term (Passiamo ad un nuovo Term)
- Change to Candidate state
- Vote for self
- Send RequestVote RPCs to all other servers, retry until either:
1. Receive votes from majority of servers:
	- Become leader
	- Send AppendEntries heartbeats to all other servers
2. Receive RPC from valid leader:
	- Return to follower state
3. No-one wins election (election timeout elapses):
	- Increment, start new election

- **Safety**: Allow at most one winner per term
	- Each server gives out only one vote per term
	- Two different candidates can’t accumulate majorities in same term
	-  Each server gives out only one vote per term
	- Split vote $\to$ no one wins.

![[296.png]]
![[297.png]]

- **Liveness**: Some candidate must eventually win
	- Choose election timeouts randomly in [T, 2T]
	- One server usually times out and wins election before others wake up
	- NB: THE SYSTEM IS ASSUMED EVENTUALLY-SYNC (Why?)

![[294.png]]
![[295.png]]
## Log structure
```ad-abstract
title: Definition
Log is an **append only list** that every replica has, including the primary (leader). On this log everyone append comands.

```

The log structure is composed by:
- index of the log (1, 2, 3, 4, 5, ...)
- each entry contains:
	- term number = term in which this entry was generated (1 means that was generated by the leader of round one on term one)
	- Command we want to execute (depends on the object)

```ad-question
È possibile che nel secondo log index, i due sistemi hanno stesso term number ma diverse operazioni? No, perchè il leader quando fa un'operazione manda la copia del suo log a tutti gli altri replica e ottiene indietro tutti gli ACK.

```

The primary system (leader) reply to a Client when the operation is commited!

```ad-abstract
title: Committed

Means that the operation has been executed by your state machine and whatever happens in the future, even if you die and someone else take over whatever that operation has been executed in a certain order and everyone will see that operation in that exact position.
```

**When is committed**? 
We send our broadcast to everyone and then you will wait until the replica say "okay, i appended the command". I wait for a majority of acks back. 

```ad-important

In raft an entry is committed when is on a majority of server.
```

Abbiamo bisogno di una majority perchè se io invio il comando ai server replica questi lo visualizzano e poi si disconnettono il comando non è stato salvato.
How many process can die? a minority!

>This definition of committed works only if the primary does not die. But for now it's fine.

```ad-question
Quando passiamo ad un nuovo term, i systems replica non dovrebbero avere tutti lo stesso contenuto? Il candidato che viene eletto come leader sarà quello che ha la log structure più aggiornata. Anche se non sono sincronizzate perfettamente tutte, comunque scegliamo, come leader, quella più aggiornata che contiene tutto!

```

![[Pasted image 20250130160230.png]]

>**Safety**: if a command is committed, every leader we elect in the future will contains all the committed entry.

Nella foto sopra, il leader ($P1$), $P2$ e $P3$ hanno tutte e tre fatto il commit del term number 3 con l'operazione di `shl`. Questo significa che, ogni leader che verrà eletto in futuro conterrà tutte le entry che sono state committate fino a questa qua (compresa).

Leader always send operation and command in the same order (FIFO)!

This pattern:
- For election phase, a process need a majority
- For commit a command, the leader need another majority
We see it already in: `Paxos` (two phases algorithm)


## Normal operation when leader is alive
- Client sends command to leader
-  Leader appends command to its log
-  Leader sends `AppendEntries RPCs` to followers
- Once new entry committed:
	- Leader passes command to its state machine, returns result to client
	- Leader notifies followers of committed entries in subsequent
	- AppendEntries RPCs Followers pass committed commands to their state machines
-  Crashed/slow followers?
	- Leader retries RPCs
- Performance is optimal (leader not die) in common case:
	- One successful RPC to any majority of servers

>If repica fail don't happend nothing, wake up in future and will eventually receive all in the right order.

How many message we generate in raft to commit and satisfy a single operation of a client if the leader does not die? $2N$ to reach and acks back.

>The cost: Election cost + 2N $\cdot$ number of requests of clients ($N$)

## Raft preserves Log consistency
The property ensures high level of coherency between logs: 
- If log entries on different servers have same index and term: 
	- They store the same command (!!)
	- The logs are identical in all preceding entries (!!)

![[298.png]]

È possibile avere una cose del genere come in foto (quadratino blu e rosso)? Yes, it is possible when:
- first message is slow in the network
- someone,not client 2, started the election for term 4
	- Someone won as a leader

In the image, `div 3` is not committed!

>If a given entry is committed, all preceding entries are also committed

![[299.png|570]]


## AppendEntries Consistency Check
- Each `AppendEntries RPC` contains index, term of entry preceding new ones
- Follower must contain matching entry; otherwise it rejects request
- Implements an induction step, ensures coherency

![[300.png]]

```ad-info
Il leader quando vuole fare il commit di un comando come, per esempio, `jump 3` manda in broadcast a tutti il term number ed il comando ma ad una condizione ovvero: "se e solo se il commit precedente è: `2 mov`"

```


## Leader Changes
At beginning of new leader’s term:
- Old leader may have left entries partially replicated
-  No special steps by new leader: just start normal operation
-  Leader’s log is “the truth”
- Will eventually make follower’s logs identical to leader’s
-  Multiple crashes can leave many extraneous log entries:

![[301.png]]
we cannot have a system that elect $s_4$ or $s_5$ as a leader.
Because if we do an election system in which $s_4$ become the leader , he will try to erase this entry (5,5,5 in column) and put 2. This is not good because if someone is committed we do a response back to the client (we cannot roll back)


>COMMITED ENTRIES HAVE TO NOT BE OVERWRITTEN

## Safety Requirement
Once a log entry has been applied to a state machine (was committed on a majority), no other state machine must apply a different value for that log entry.

>If we commit in the log index the term id $5$ is not possible that someone, in future, commit another term id 2! **Questo andrebbe a rompere la proprietà di safety**

==Raft safety property==:
- If a leader has decided that a log entry is committed, that entry will be present in the logs of all future leaders. Thanks:
	- new definition of committed
	- when something is commited it must influence the leader election
This guarantees the safety requirement
- Leaders never overwrite entries in their logs
- Only entries in the leader’s log can be committed
- Entries must be committed before applying to state machine

![[302.png]]

## Picking the best Leader
Can’t tell which entries are committed!


![[303.png]]

We need to avoid that the second become the leader. Altrimenti potrebbe fare il commit di un valore e sostituirlo sugli altri due, cosa che non deve succedere!

$C_2$ vuole essere eletto quindi diventa un candidato e chiede a gli altri due di votarlo. $C_1$ confronta la sua log structure con quella del candidato $C_2$ e vede che la sua è più aggiornata dunque capisce che non deve dargli il suo voto. Quindi $C_1$ nega il voto a $C_2$ 


During elections, choose candidate with log most likely to contain all committed entries
- Candidates include log info in RequestVote RPCs (index & term of last log entry)
- Voting server V denies vote if its log is “more complete”: 
	- `(lastTermV > lastTermC) || (lastTermV == lastTermC) && (lastIndexV > lastIndexC)`
- Leader will have “most complete” log among electing majority

### Committing Entry from Current Term
Case `#1/2`: Leader decides entry in current term is committed
![[304.png]]

>Safe: leader for term 3 must contain entry 4 (S4 ed S5 non possono essere votati!)


Case `#2/2`: Leader is trying to finish committing entry from an earlier term
![[305.png]]

```ad-info

Se S1 fallisce al term 4, qual'è che viene eletto? viene eletto S5 perchè term3 è maggiore di term2. Quindi anche se al term 2 sono in maggioranza S1, S2 e S3, nel momento del failure S5 è quello con il term più grande prima di 4
```

Entry $3$ not safely committed:
- $s_5$ can be elected as leader for term $5$
- If elected, it will overwrite entry $3$ on $s_1$, $s_2$, and $s_3$!

## New committed rules
For a leader to decide an entry is committed:
- Must be stored on a majority of servers
- At least one new entry from leader’s term must also be stored on majority of servers

Once entry 4 committed:
- $s_5$ cannot be elected leader for term $5$
- Entries $3$ and $4$ both safe

![[306.png]]

> Combination of election rules and commitment rules makes Raft safe

### Log Inconsistencies
Leader changes can result in log inconsistencies:

![[307.png]]

## Repairing Follower Logs
New leader must make follower logs consistent with its own
- Delete extraneous entries
- Fill in missing entries
Leader keeps nextIndex for each follower:
- Index of next log entry to send to that follower
- Initialized to (1 + leader’s last index)
When AppendEntries consistency check fails, decrement nextIndex and try again:

![[308.png]]

When follower overwrites inconsistent entry, it deletes all subsequent entries:

![[309.png]]

## Neutralizing Old Leader
Deposed leader may not be dead:
- Temporarily disconnected from network
- Other servers elect a new leader
- Old leader becomes reconnected, attempts to commit log entries
Terms used to detect stall leaders (and candidates)
- Every RPC contains term of sender
- If sender’s term is older, RPC is rejected, sender reverts to follower and updates its term
- If receiver’s term is older, it reverts to follower, updates its term, then processes RPC normally
Election updates terms of majority of servers
- Deposed server cannot commit new log entries

## Client Protocol
Send commands to leader
- If leader unknown, contact any server
- If contacted server not leader, it will redirect to leader

Leader does not respond until command has been logged, committed, and executed by leader’s state machine.

 If request times out (e.g., leader crash):
- Client reissues command to some other server
- Eventually redirected to new leader
- Retry request with new leader

What if leader crashes after executing command, but before responding?
- Must not execute command twice
Solution: client embeds a unique id in each command
- Server includes id in log entry
- Before accepting command, leader checks its log for entry with that id
- If id found in log, ignore new command, return response from oldcommand

**Result**: exactly-once semantics as long as client doesn’t crash